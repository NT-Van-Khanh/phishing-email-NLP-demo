{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "eY1C50dpJzKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mailbox\n",
        "import email\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# import gensim\n",
        "import gensim.parsing.preprocessing as preprocessing\n",
        "import gensim.corpora as corpora\n",
        "import gensim.models as models\n",
        "# from gensim.models import Word2Vec\n",
        "# from gensim.models import LdaMulticore\n",
        "\n",
        "\n",
        "import sklearn.feature_extraction.text as text"
      ],
      "metadata": {
        "id": "je-Elkn0J3nU",
        "outputId": "5c55c586-52f0-4068-c9d7-052bcaae0121",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "YraJJFnTQwvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Email"
      ],
      "metadata": {
        "id": "5ZEwrhkOWzXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions: Extract email from .mbox file and .eml file\n"
      ],
      "metadata": {
        "id": "YBf_LlOI-Cnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_emails_from_mbox(mbox_file_name):\n",
        "  messages=[]\n",
        "  try:\n",
        "    mbox = mailbox.mbox(mbox_file_name)\n",
        "    for message in mbox:\n",
        "      messages.append(message)\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File not found: {mbox_file_name}\")\n",
        "  return messages\n",
        "\n",
        "\n",
        "def extract_email_from_eml(eml_file_name):\n",
        "  email_message=None\n",
        "  try:\n",
        "      with open(eml_file_name, \"r\") as email_file:\n",
        "          email_message = email.message_from_file(email_file)\n",
        "  except FileNotFoundError:\n",
        "      print(f\"File not found: {eml_file_name}\")\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "  return email_message\n",
        "\n",
        "#From:https://stackoverflow.com/questions/7166922/extracting-the-body-of-an-email-from-mbox-file-decoding-it-to-plain-text-regard"
      ],
      "metadata": {
        "id": "M0vGJHcsbPak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions: Extract component of email"
      ],
      "metadata": {
        "id": "mJYw16uQIDEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sender_email(email_message):\n",
        "  return email_message.get(\"From\")\n",
        "\n",
        "\n",
        "def extract_subject_email(email_message):\n",
        "  return email_message.get(\"Subject\")\n",
        "\n",
        "\n",
        "def extract_content_email(email_message):\n",
        "  body = None\n",
        "  if(email_message.is_multipart()):\n",
        "    for part in email_message.walk():\n",
        "      if(part.is_multipart()):\n",
        "        for subpart in part.walk():\n",
        "          if(subpart.get_content_type() == \"text/plain\"):\n",
        "            body = subpart.get_payload(decode=True)\n",
        "          # elif(subpart.get_content_type() == \"text/html\"):\n",
        "          #   body = subpart.get_payload(decode=True)\n",
        "      elif(part.get_content_type() == \"text/plain\"):\n",
        "        body = part.get_payload(decode=True)\n",
        "  else:\n",
        "    body = email_message.get_payload(decode=True)\n",
        "  if(body is not None):\n",
        "    #chuyển dữ liệu dạng byte string sang string (utf-8)\n",
        "    body=body.decode('utf-8')\n",
        "  return body"
      ],
      "metadata": {
        "id": "sBT5Yl7GTPfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process: Extract phishing emails"
      ],
      "metadata": {
        "id": "HtC14PZmJG_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phishing_message_bodies = []\n",
        "\n",
        "\n",
        "phishing_messages = extract_emails_from_mbox(\"/content/emails-enron-legal-mails.mbox\")\n",
        "for message in phishing_messages:\n",
        "  body=extract_content_email(message)\n",
        "  if (body is not None and body.strip()):\n",
        "    phishing_message_bodies.append(body)\n",
        "    #print(body)\n",
        "    #print(body).decode('utf-8')\n",
        "\n",
        "print(len(phishing_message_bodies))\n",
        "print(len(phishing_messages))"
      ],
      "metadata": {
        "id": "b2FoHAWSGrUw",
        "outputId": "7e95efbf-39ae-48da-d1df-0eb4b0bc4b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4279\n",
            "4279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process: Extract benign emails"
      ],
      "metadata": {
        "id": "dYVDbnwbEUie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benign_message_bodies = []\n",
        "\n",
        "\n",
        "benign_messages = extract_emails_from_mbox(\"/content/emails-enron-ham.mbox\")\n",
        "for message in benign_messages:\n",
        "  body=extract_content_email(message)\n",
        "  if (body is not None and body.strip()):\n",
        "    benign_message_bodies.append(body)\n",
        "\n",
        "print(len(benign_message_bodies))\n",
        "print(len(benign_messages))"
      ],
      "metadata": {
        "id": "pmmlNwehETr0",
        "outputId": "352b7190-9939-424b-f440-67d7e1804e58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ===TEST==="
      ],
      "metadata": {
        "id": "VQOB1m2h1Tug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msg = extract_email_from_eml(\"/content/sample1.eml\")\n",
        "messages = extract_emails_from_mbox(\"/emails-enron-legal-mails.mbox\")\n",
        "if(msg):\n",
        "  print(extract_sender_email(msg))\n",
        "  print(extract_subject_email(msg))\n",
        "  print(extract_content_email(msg))"
      ],
      "metadata": {
        "id": "LzTM6l8fKWtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(phishing_messages[0])\n",
        "print(phishing_message_bodies[0])"
      ],
      "metadata": {
        "id": "ZFt1xuQ61WBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(benign_messages[0])\n",
        "print(benign_message_bodies[0])"
      ],
      "metadata": {
        "id": "3t1IbwnB1Z2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple preprocessing"
      ],
      "metadata": {
        "id": "jdd-9_Ye04iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom stop words and preprocessing filters"
      ],
      "metadata": {
        "id": "V3njuzeQQOvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom stop words and preprocessing filters\n",
        "stopWords = nltk.corpus.stopwords\n",
        "stopWords = stopWords.words(\"english\")\n",
        "stopWords.extend([\"nbsp\", \"font\", \"sans\", \"serif\", \"bold\", \"arial\", \"verdana\", \"helvetica\", \"http\", \"https\", \"www\", \"html\", \"enron\", \"margin\", \"spamassassin\"])\n",
        "\n",
        "def remove_custom_stopwords(p):\n",
        "    return preprocessing.remove_stopwords(p, stopwords=stopWords)\n",
        "\n",
        "CUSTOM_FILTERS = [lambda x: x.lower(), preprocessing.strip_tags, preprocessing.strip_punctuation,\n",
        "                  preprocessing.strip_multiple_whitespaces, preprocessing.strip_numeric, remove_custom_stopwords,\n",
        "                  preprocessing.remove_stopwords, preprocessing.strip_short, preprocessing.stem_text]\n"
      ],
      "metadata": {
        "id": "ZftALxyg079z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_preprocessing(messages):\n",
        "  preprocessed_messages = []\n",
        "  for message in messages:\n",
        "    preprocessed = preprocessing.preprocess_string(message,filters = CUSTOM_FILTERS)\n",
        "    if preprocessed and (preprocessed not in preprocessed_messages):\n",
        "      preprocessed_messages.append(preprocessed)\n",
        "\n",
        "  return preprocessed_messages"
      ],
      "metadata": {
        "id": "UHD1wYOMJUom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess messages"
      ],
      "metadata": {
        "id": "18F0fKdqQTY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing phishing message bodies\n",
        "phishing_preprocessed = []\n",
        "phishing_preprocessed =  custom_preprocessing(phishing_message_bodies)\n",
        "\n",
        "print(len(phishing_preprocessed))"
      ],
      "metadata": {
        "id": "MkbJl5lZ0_SL",
        "outputId": "985035ad-d47f-4acd-deac-5e9580bdde9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing benign message bodies\n",
        "benign_preprocessed = []\n",
        "benign_preprocessed =  custom_preprocessing(benign_message_bodies)\n",
        "\n",
        "print(len(benign_preprocessed))"
      ],
      "metadata": {
        "id": "t2FnvYnt1BWr",
        "outputId": "fb811965-9792-4482-8556-658028a95cf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ===TEST==="
      ],
      "metadata": {
        "id": "BWgiCUiuQn7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "print(phishing_preprocessed[1])"
      ],
      "metadata": {
        "id": "_0U5tciJ1DKp",
        "outputId": "33ed6429-d684-45ee-9309-8cd08d0e7b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['trade', 'profit', 'allen', 'grigsbi', 'rest', 'desk', 'total', 'view', 'bonu', 'partli', 'attribut', 'trade', 'partli', 'group', 'perform', 'thought', 'minimum', 'market', 'maximum', 'cash', 'equiti', 'mike', 'number', 'adjust', 'minimum', 'market', 'maximum', 'cash', 'equiti', 'given', 'expect', 'speech', 'point', 'phillip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec Embedding\n"
      ],
      "metadata": {
        "id": "wtANJcvfjMD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on all messages\n",
        "model = models.Word2Vec(phishing_preprocessed + benign_preprocessed, vector_size=100, min_count=1, workers=3, window=5)\n",
        "\n",
        "#From: https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/"
      ],
      "metadata": {
        "id": "kRgK9ldSkCtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"dollar\", topn=20)"
      ],
      "metadata": {
        "id": "3e4GPHLjluvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv[\"dollar\"]"
      ],
      "metadata": {
        "id": "ykaw9PxJlvup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA Topic Modeling"
      ],
      "metadata": {
        "id": "j8sFzX6jU8mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init the number of topics"
      ],
      "metadata": {
        "id": "ZpBNSKMUWhQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numTopics = 1024"
      ],
      "metadata": {
        "id": "_d1CZXaGVRy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dictionary and corpus"
      ],
      "metadata": {
        "id": "aguwb1mdVu4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_message_preprocessed = phishing_preprocessed + benign_preprocessed"
      ],
      "metadata": {
        "id": "r6qsxlGvVccA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(all_message_preprocessed)\n",
        "corpus = [dictionary.doc2bow(text) for text in all_message_preprocessed]"
      ],
      "metadata": {
        "id": "5aw2MNILTy1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Create LDA model"
      ],
      "metadata": {
        "id": "efVePDN9VySO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LDA_model = models.LdaMulticore(corpus=corpus, num_topics=numTopics, id2word=dictionary)"
      ],
      "metadata": {
        "id": "TbqOjNTPXTVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "# Print keyword for the topics\n",
        "print(LDA_model.print_topics())"
      ],
      "metadata": {
        "id": "SNJqzs9mZ3wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Doc2Vec"
      ],
      "metadata": {
        "id": "K-V4GinQabbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_data = [models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(all_message_preprocessed)]"
      ],
      "metadata": {
        "id": "aC8vWlZ3aezK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Khởi tạo và huấn luyện trực tiếp\n",
        "doc2vec_model = models.Doc2Vec(tagged_data, vector_size=20, window=2, min_count=1, workers=4)\n",
        "\n",
        "#Tách khởi tạo và huấn luyện1\n",
        "# doc2vec_model = models.Doc2Vec(tagged_data, vector_size=100, min_count=1, epochs=10)\n",
        "# doc2vec_model.build_vocab(tagged_data)\n",
        "# doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)"
      ],
      "metadata": {
        "id": "5_JOKTTzfDMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classification"
      ],
      "metadata": {
        "id": "EDQ0WAcNK4Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_message_bodies = phishing_message_bodies + benign_message_bodies"
      ],
      "metadata": {
        "id": "04yicyRHSvSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_lines(file_path):\n",
        "  lines=[]\n",
        "  try:\n",
        "    with open(file_path, 'r') as file:\n",
        "      for line in file:\n",
        "        lines.append(line.strip())\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File not found: {file_path}\")\n",
        "  return lines"
      ],
      "metadata": {
        "id": "h9VMJIFaDX3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "black_list_words = get_file_lines(\"/content/spam_wordlist.txt\")\n",
        "black_list = custom_preprocessing(black_list_words)\n",
        "\n",
        "print(len(black_list_words))\n",
        "print(len(black_list))"
      ],
      "metadata": {
        "id": "D87FfTy6Rj9T",
        "outputId": "3f0833aa-40c3-4532-da86-ccce1d999f79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "582\n",
            "369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF\n",
        "max_term=6"
      ],
      "metadata": {
        "id": "SpzXq0S4VWxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_string(lst):\n",
        "    return ' '.join(lst)\n",
        "\n",
        "def count_all_upper_words(text):\n",
        "    count = 0\n",
        "    for word in text.split():\n",
        "        if word.isupper():\n",
        "            count += 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "x0PWX0K9ctOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidfVectorizer = text.TfidfVectorizer(max_features=max_term, preprocessor=list_to_string,sublinear_tf=True)\n",
        "tfidf_matrix = tfidfVectorizer.fit_transform(all_message_preprocessed).toarray()\n",
        "\n",
        "print(tfidfVectorizer.get_feature_names_out())\n",
        "print(tfidf_matrix.shape)"
      ],
      "metadata": {
        "id": "eRfu_3cSYF0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allVectors = []\n",
        "\n",
        "for i in range(all_message_bodies):\n",
        "  top_topics = LDA_model.get_document_topics(corpus[i], minimum_probability=0.0)\n",
        "  vec =  [top_topics[i][1] for i in range(numTopics)]#topics\n",
        "\n",
        "  for v in doc2vec_model.infer_vector(all_message_preprocessed[i]):#Doc2Vec--need to fix\n",
        "    vec.append(v)\n",
        "\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  sentence = \" \".join(all_message_preprocessed[i])\n",
        "  polarity = sia.polarity_scores(sentence)\n",
        "\n",
        "  for s in polarity:\n",
        "    vec.append(polarity[s])\n",
        "\n",
        "  # Contains HTML\n",
        "  if \"<html>\" in allBodies[i].lower():\n",
        "      vec.append(1)\n",
        "  else:\n",
        "      vec.append(0)\n",
        "\n",
        "  # Contains a link (how many)\n",
        "  if \"http://\" in allBodies[i].lower() or \"https://\" in allBodies[i].lower():\n",
        "      vec.append(1)\n",
        "  else:\n",
        "      vec.append(0)\n",
        "\n",
        "  # How many blacklisted phrases/words appear in this email\n",
        "  for b in blackList:\n",
        "      count = 0\n",
        "      for word in b:\n",
        "          if word in allPreprocessed[i]:\n",
        "              count += 1\n",
        "      vec.append(count)\n",
        "\n",
        "  # TF-IDF for top terms\n",
        "  for w in tfIDF[i]:\n",
        "      vec.append(w)\n",
        "\n",
        "  # Has all caps word?\n",
        "  vec.append(count_all_upper_word(allBodies[i]))\n",
        "\n",
        "  # Has exclamation marks?\n",
        "  vec.append(allBodies[i].count(\"!\"))\n",
        "\n",
        "  # Total length\n",
        "  vec.append(len(allBodies[i]))\n",
        "\n",
        "  # Num words\n",
        "  vec.append(len(allPreprocessed[i]))\n",
        "\n",
        "\n",
        "  allVectors.append(vec)\n"
      ],
      "metadata": {
        "id": "3EPnNSZ2It9e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}