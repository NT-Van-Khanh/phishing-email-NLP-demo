{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Import"
      ],
      "metadata": {
        "id": "eY1C50dpJzKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mailbox\n",
        "import email\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "import nltk.sentiment as sentiment                          #from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# import gensim\n",
        "import gensim.parsing.preprocessing as gs_preprocessing     #from gensim.parsing.preprocessing import *\n",
        "import gensim.corpora as corpora\n",
        "import gensim.models as models                              #from gensim.models import Word2Vec,LdaMulticore  #from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# import sklearn\n",
        "import sklearn.feature_extraction.text as text              #from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sklearn.model_selection as model_selection           #from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "import sklearn.preprocessing as sk_preprocessing            #from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import sklearn.ensemble as ensemble\n",
        "import sklearn.metrics as metrics                           #from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import sklearn.svm as svm                                   #from sklearn.svm import SVC\n",
        "\n",
        "import tensorflow.config as tf_config                       #from tensorflow import config\n",
        "import tensorflow.keras as tf_keras                         #from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "je-Elkn0J3nU",
        "outputId": "647dd97f-ca0b-4c82-9153-4c65ba181b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "YraJJFnTQwvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Explore Data"
      ],
      "metadata": {
        "id": "5ZEwrhkOWzXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions: Extract email from .mbox file and .eml file\n"
      ],
      "metadata": {
        "id": "YBf_LlOI-Cnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_emails_from_mbox(mbox_file_name):\n",
        "  messages=[]\n",
        "  try:\n",
        "    mbox = mailbox.mbox(mbox_file_name)\n",
        "    for message in mbox:\n",
        "      messages.append(message)\n",
        "    # messages = [m[1] for m in mbox.items()]\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File not found: {mbox_file_name}\")\n",
        "  return messages\n",
        "\n",
        "\n",
        "def extract_email_from_eml(eml_file_name):\n",
        "  email_message=None\n",
        "  try:\n",
        "      with open(eml_file_name, \"r\") as email_file:\n",
        "          email_message = email.message_from_file(email_file)\n",
        "  except FileNotFoundError:\n",
        "      print(f\"File not found: {eml_file_name}\")\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "  return email_message\n",
        "\n",
        "#From:https://stackoverflow.com/questions/7166922/extracting-the-body-of-an-email-from-mbox-file-decoding-it-to-plain-text-regard"
      ],
      "metadata": {
        "id": "M0vGJHcsbPak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions: Extract component of email"
      ],
      "metadata": {
        "id": "mJYw16uQIDEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sender_email(email_message):\n",
        "  return email_message.get(\"From\")\n",
        "\n",
        "\n",
        "def extract_subject_email(email_message):\n",
        "  return email_message.get(\"Subject\")\n",
        "\n",
        "\n",
        "def extract_content_email(email_message):\n",
        "  body = None\n",
        "  if(email_message.is_multipart()):\n",
        "    for part in email_message.walk():\n",
        "      if(part.is_multipart()):\n",
        "        for subpart in part.walk():\n",
        "          if(subpart.get_content_type() == \"text/plain\"):\n",
        "            body = subpart.get_payload(decode=True)\n",
        "          # elif(subpart.get_content_type() == \"text/html\"):\n",
        "          #   body = subpart.get_payload(decode=True)\n",
        "      elif(part.get_content_type() == \"text/plain\"):\n",
        "        body = part.get_payload(decode=True)\n",
        "  else:\n",
        "    body = email_message.get_payload(decode=True)\n",
        "  if(body is not None) and isinstance(body, bytes):\n",
        "    #chuyển dữ liệu dạng byte string sang string (utf-8)\n",
        "    try:\n",
        "      body=body.decode('utf-8', errors='ignore')\n",
        "    except UnicodeDecodeError:\n",
        "      # Không làm gì cả nếu lỗi xảy ra, giữ nguyên giá trị body\n",
        "      pass\n",
        "  return body"
      ],
      "metadata": {
        "id": "sBT5Yl7GTPfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process: Extract phishing emails"
      ],
      "metadata": {
        "id": "HtC14PZmJG_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phishing_message_bodies = []\n",
        "\n",
        "phishing_messages = extract_emails_from_mbox(\"/content/fradulent_emails.txt\")\n",
        "for message in phishing_messages:\n",
        "  body=extract_content_email(message)\n",
        "  if (body is not None and body.strip()):\n",
        "    phishing_message_bodies.append(body)\n",
        "    #print(body)\n",
        "    #print(body).decode('utf-8')\n",
        "\n",
        "print(len(phishing_message_bodies))\n",
        "print(len(phishing_messages))"
      ],
      "metadata": {
        "id": "b2FoHAWSGrUw",
        "outputId": "d23e56f6-337b-4866-f729-80e074526028",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4279\n",
            "4279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process: Extract benign emails"
      ],
      "metadata": {
        "id": "dYVDbnwbEUie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benign_message_bodies = []\n",
        "\n",
        "\n",
        "benign_messages = extract_emails_from_mbox(\"/content/emails-enron-legal-mails.mbox\")\n",
        "for message in benign_messages:\n",
        "  body=extract_content_email(message)\n",
        "  if (body is not None and body.strip()):\n",
        "    benign_message_bodies.append(body)\n",
        "\n",
        "print(len(benign_message_bodies))\n",
        "print(len(benign_messages))"
      ],
      "metadata": {
        "id": "pmmlNwehETr0",
        "outputId": "fc2dd9ab-4099-4589-80aa-fd7fc9097a53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ===TEST==="
      ],
      "metadata": {
        "id": "VQOB1m2h1Tug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msg = extract_email_from_eml(\"/content/sample1.eml\")\n",
        "messages = extract_emails_from_mbox(\"/emails-enron-legal-mails.mbox\")\n",
        "if(msg):\n",
        "  print(extract_sender_email(msg))\n",
        "  print(extract_subject_email(msg))\n",
        "  print(extract_content_email(msg))"
      ],
      "metadata": {
        "id": "LzTM6l8fKWtI",
        "outputId": "c3289350-f9b0-4d8c-8132-000375bdbfd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found: /content/sample1.eml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(phishing_messages[0])\n",
        "print(phishing_message_bodies[0])"
      ],
      "metadata": {
        "id": "ZFt1xuQ61WBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(benign_messages[0])\n",
        "print(benign_message_bodies[0])"
      ],
      "metadata": {
        "id": "3t1IbwnB1Z2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple preprocessing"
      ],
      "metadata": {
        "id": "jdd-9_Ye04iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom stop words and preprocessing filters"
      ],
      "metadata": {
        "id": "V3njuzeQQOvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom stop words and preprocessing filters\n",
        "stopWords = nltk.corpus.stopwords\n",
        "stopWords = stopWords.words(\"english\")\n",
        "stopWords.extend([\"nbsp\", \"font\", \"sans\", \"serif\", \"bold\", \"arial\", \"verdana\", \"helvetica\", \"http\", \"https\", \"www\", \"html\", \"enron\", \"margin\", \"spamassassin\"])\n",
        "\n",
        "def remove_custom_stopwords(p):\n",
        "    return gs_preprocessing.remove_stopwords(p, stopwords=stopWords)\n",
        "\n",
        "CUSTOM_FILTERS = [lambda x: x.lower(), gs_preprocessing.strip_tags, gs_preprocessing.strip_punctuation,\n",
        "                  gs_preprocessing.strip_multiple_whitespaces, gs_preprocessing.strip_numeric, remove_custom_stopwords,\n",
        "                  gs_preprocessing.remove_stopwords, gs_preprocessing.strip_short, gs_preprocessing.stem_text]\n"
      ],
      "metadata": {
        "id": "ZftALxyg079z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_preprocessing(messages):\n",
        "  preprocessed_messages = []\n",
        "  for message in messages:\n",
        "    preprocessed = gs_preprocessing.preprocess_string(message,filters = CUSTOM_FILTERS)\n",
        "    #NEED FIX: xu ly trung lap chi can thiet o black list\n",
        "    # if preprocessed and (preprocessed not in preprocessed_messages):\n",
        "    #   preprocessed_messages.append(preprocessed)\n",
        "    preprocessed_messages.append(preprocessed)\n",
        "\n",
        "  return preprocessed_messages\n",
        "\n",
        "#Bỏ các phần tử rỗng và trùng lập\n",
        "def duplicate_filter(texts):\n",
        "    unique_texts = []\n",
        "    for text in texts:\n",
        "        if text and (text not in unique_texts):\n",
        "            unique_texts.append(text)\n",
        "    return unique_texts\n"
      ],
      "metadata": {
        "id": "UHD1wYOMJUom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess messages"
      ],
      "metadata": {
        "id": "18F0fKdqQTY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing phishing message bodies\n",
        "phishing_preprocessed = []\n",
        "phishing_preprocessed =  custom_preprocessing(phishing_message_bodies)\n",
        "\n",
        "print(len(phishing_preprocessed))"
      ],
      "metadata": {
        "id": "MkbJl5lZ0_SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing benign message bodies\n",
        "benign_preprocessed = []\n",
        "benign_preprocessed =  custom_preprocessing(benign_message_bodies)\n",
        "\n",
        "print(len(benign_preprocessed))"
      ],
      "metadata": {
        "id": "t2FnvYnt1BWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ===TEST==="
      ],
      "metadata": {
        "id": "BWgiCUiuQn7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "print(len(phishing_message_bodies))\n",
        "print(len(duplicate_filter(phishing_message_bodies)))\n",
        "print(len(phishing_preprocessed))\n",
        "print(len(duplicate_filter(phishing_preprocessed)))\n",
        "\n",
        "print(phishing_preprocessed[0])"
      ],
      "metadata": {
        "id": "_0U5tciJ1DKp",
        "outputId": "382120c9-af60-4508-f04a-26b2f16d90e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4279\n",
            "4246\n",
            "4279\n",
            "4088\n",
            "['pai', 'pai', 'perform', 'bonus', 'base', 'merit', 'entitl', 'soundbit', 'prc', 'email', 'get', 'worri', 'mike', 'open', 'discuss', 'phillip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "gxcFSOInJrpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_message_preprocessed = phishing_preprocessed + benign_preprocessed\n",
        "all_message_bodies = phishing_message_bodies + benign_message_bodies\n",
        "\n",
        "print(len(all_message_preprocessed))\n",
        "print(len(all_message_bodies))"
      ],
      "metadata": {
        "id": "r6qsxlGvVccA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec Embedding\n"
      ],
      "metadata": {
        "id": "wtANJcvfjMD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on all messages\n",
        "word2vec_model = models.Word2Vec(all_message_preprocessed, vector_size=100, min_count=1, workers=3, window=5)\n",
        "#From: https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/"
      ],
      "metadata": {
        "id": "kRgK9ldSkCtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.wv.most_similar(\"dollar\", topn=20)"
      ],
      "metadata": {
        "id": "3e4GPHLjluvu",
        "outputId": "23d0d51c-cd0d-4fbc-d0f9-915970d8236e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('billion', 0.9507895112037659),\n",
              " ('win', 0.9110546112060547),\n",
              " ('ubid', 0.903036892414093),\n",
              " ('maximum', 0.8959468603134155),\n",
              " ('tend', 0.8940055966377258),\n",
              " ('quantiti', 0.8918537497520447),\n",
              " ('supersit', 0.8888633251190186),\n",
              " ('defici', 0.8846665024757385),\n",
              " ('prioriti', 0.8796712160110474),\n",
              " ('half', 0.8794583082199097),\n",
              " ('auction', 0.8794552087783813),\n",
              " ('size', 0.8770312666893005),\n",
              " ('dissent', 0.8762837648391724),\n",
              " ('entri', 0.8752008080482483),\n",
              " ('dcq', 0.8745139837265015),\n",
              " ('store', 0.8734740018844604),\n",
              " ('majeur', 0.8722878098487854),\n",
              " ('grab', 0.8720036149024963),\n",
              " ('shorter', 0.8716081380844116),\n",
              " ('furthermor', 0.8704047203063965)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.wv[\"dollar\"]"
      ],
      "metadata": {
        "id": "ykaw9PxJlvup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA Topic Modeling"
      ],
      "metadata": {
        "id": "j8sFzX6jU8mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Init the number of topics"
      ],
      "metadata": {
        "id": "ZpBNSKMUWhQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numTopics = 1024"
      ],
      "metadata": {
        "id": "_d1CZXaGVRy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dictionary and corpus"
      ],
      "metadata": {
        "id": "aguwb1mdVu4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(all_message_preprocessed)\n",
        "corpus = [dictionary.doc2bow(text) for text in all_message_preprocessed]"
      ],
      "metadata": {
        "id": "5aw2MNILTy1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "iterator = iter(dictionary.items())\n",
        "print([next(iterator) for _ in range(6)])\n",
        "print(corpus[0])"
      ],
      "metadata": {
        "id": "YPhs2VaHPZqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Create LDA model"
      ],
      "metadata": {
        "id": "efVePDN9VySO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LDA_model = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=numTopics)"
      ],
      "metadata": {
        "id": "TbqOjNTPXTVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "# Print keyword for the topics\n",
        "print(LDA_model.print_topics())"
      ],
      "metadata": {
        "id": "SNJqzs9mZ3wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Doc2Vec Embedding"
      ],
      "metadata": {
        "id": "K-V4GinQabbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_data = [models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(all_message_preprocessed)]"
      ],
      "metadata": {
        "id": "aC8vWlZ3aezK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Khởi tạo và huấn luyện trực tiếp\n",
        "doc2vec_model = models.Doc2Vec(tagged_data, vector_size=20, window=2, min_count=1, workers=4)\n",
        "\n",
        "#Tách khởi tạo và huấn luyện1\n",
        "# doc2vec_model = models.Doc2Vec(tagged_data, vector_size=100, min_count=1, epochs=10)\n",
        "# doc2vec_model.build_vocab(tagged_data)\n",
        "# doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)"
      ],
      "metadata": {
        "id": "5_JOKTTzfDMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blacklist words"
      ],
      "metadata": {
        "id": "QeuddVD5-Knt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_lines(file_path):\n",
        "  lines=[]\n",
        "  try:\n",
        "    with open(file_path, 'r') as file:\n",
        "      for line in file:\n",
        "        lines.append(line.strip())\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File not found: {file_path}\")\n",
        "  return lines"
      ],
      "metadata": {
        "id": "h9VMJIFaDX3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "black_list_words = get_file_lines(\"/content/spam_wordlist.txt\")\n",
        "black_list = custom_preprocessing(black_list_words)\n",
        "black_list = duplicate_filter(black_list)\n",
        "\n",
        "print(len(black_list_words))\n",
        "print(len(black_list))"
      ],
      "metadata": {
        "id": "D87FfTy6Rj9T",
        "outputId": "6303c62b-94d5-4d5a-f0d3-1d964009d399",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "582\n",
            "369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "00E_wzpVqh8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF\n",
        "max_term=6"
      ],
      "metadata": {
        "id": "SpzXq0S4VWxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_string(lst):\n",
        "    return ' '.join(lst)\n",
        "\n",
        "def count_all_upper_words(text):\n",
        "    count = 0\n",
        "    for word in text.split():\n",
        "        if word.isupper():\n",
        "            count += 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "x0PWX0K9ctOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidfVectorizer = text.TfidfVectorizer(max_features=max_term, preprocessor=list_to_string, sublinear_tf=True)\n",
        "tfidf_matrix = tfidfVectorizer.fit_transform(all_message_preprocessed).toarray()\n",
        "\n",
        "print(tfidfVectorizer.get_feature_names_out())\n",
        "print(tfidf_matrix.shape)\n",
        "print(tfidf_matrix[2])"
      ],
      "metadata": {
        "id": "eRfu_3cSYF0q",
        "outputId": "7d6ba81a-7dfa-491f-f8bc-a3845fdf23a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['com' 'content' 'ect' 'hou' 'mail' 'subject']\n",
            "(4279, 6)\n",
            "[0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function: Create Vector"
      ],
      "metadata": {
        "id": "GnnS-cy7-Etr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vectors_from_messages(messages, messages_preprocessed, messages_corpus):\n",
        "    # corpus = [dictionary.doc2bow(text) for text in messages_preprocessed] # Term document frequency\n",
        "    all_vectors = []\n",
        "    for i in range(len(messages)):\n",
        "        topTopics = LDA_model.get_document_topics(messages_corpus[i], minimum_probability=0.0)\n",
        "\n",
        "        # Can extend this array with other stuff later\n",
        "        vec = [topTopics[i][1] for i in range(numTopics)] # Topics\n",
        "\n",
        "        for v in doc2vec_model.infer_vector(messages_preprocessed[i]): # Doc2Vec\n",
        "            vec.append(v)\n",
        "\n",
        "        # Sentiment analysis of polarity\n",
        "        sia = sentiment.SentimentIntensityAnalyzer()\n",
        "        sentence = \" \".join(messages_preprocessed[i])\n",
        "        polarity = sia.polarity_scores(sentence)\n",
        "        for s in polarity:\n",
        "            vec.append(polarity[s])\n",
        "\n",
        "        # Contains HTML\n",
        "        if \"<html>\" in messages[i].lower():\n",
        "            vec.append(1)\n",
        "        else:\n",
        "            vec.append(0)\n",
        "\n",
        "        # Contains a link\n",
        "        if \"http://\" in messages[i].lower() or \"https://\" in messages[i].lower():\n",
        "            vec.append(1)\n",
        "        else:\n",
        "            vec.append(0)\n",
        "\n",
        "        # How many blacklisted phrases/words appear in this email\n",
        "        for b in black_list:\n",
        "            count = 0\n",
        "            for word in b:\n",
        "                if word in messages_preprocessed[i]:\n",
        "                    count += 1\n",
        "            vec.append(count)\n",
        "\n",
        "        # TF-IDF for top terms\n",
        "        for word_weight in tfidf_matrix[i]:\n",
        "            vec.append(word_weight)\n",
        "\n",
        "        # Has all caps word?\n",
        "        vec.append(count_all_upper_words(messages[i]))\n",
        "\n",
        "        # Has exclamation marks?\n",
        "        vec.append(messages[i].count(\"!\"))\n",
        "\n",
        "        # Total length\n",
        "        vec.append(len(messages[i]))\n",
        "\n",
        "        # Num words\n",
        "        vec.append(len(messages_preprocessed[i]))\n",
        "\n",
        "        all_vectors.append(vec)\n",
        "\n",
        "    return all_vectors"
      ],
      "metadata": {
        "id": "UD7_5yfM9Cw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process: Create vector"
      ],
      "metadata": {
        "id": "EsLsCZtGqase"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_vectors = create_vectors_from_messages(all_message_bodies, all_message_preprocessed,corpus)"
      ],
      "metadata": {
        "id": "o5aXGmxejnxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array(all_vectors).shape)\n",
        "print(all_vectors[0])"
      ],
      "metadata": {
        "id": "GVJodMRhm6NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labeling and Data Splitting"
      ],
      "metadata": {
        "id": "pJuQfnwFPJJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = []\n",
        "all_labels.extend([1]*len(phishing_preprocessed))   #for i in range(len(phishing_preprocessed)): all_labels.append(1)\n",
        "all_labels.extend([0]*len(benign_preprocessed))     #for i in range(len(benign_preprocessed)):  all_labels.append(0)"
      ],
      "metadata": {
        "id": "h3weC6JmdGj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_labels))"
      ],
      "metadata": {
        "id": "qidmJuYCUL2D",
        "outputId": "c07180b8-4181-4cae-9b3c-62fe7cdb646e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale and split data\n",
        "# Scale the data\n",
        "scaler = sk_preprocessing.StandardScaler()\n",
        "scaler.fit(all_vectors)# cần xem xét\n",
        "# Split the data into training and testing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(scaler.transform(all_vectors), all_labels, test_size=0.2, shuffle=True)\n",
        "#from: https://dagster.io/glossary/dataset-splitting"
      ],
      "metadata": {
        "id": "B6_pbP8tdTZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training data size (X_train):\" + str(X_train.shape))\n",
        "print(\"Testing data size (X_test):\" + str(X_test.shape))\n",
        "print(\"Training labels size (y_train):\" + str(y_train.shape))\n",
        "print(\"Testing labels size (y_test):\" + str(y_test.shape))"
      ],
      "metadata": {
        "id": "DmQ5mmv-Pqk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classification"
      ],
      "metadata": {
        "id": "EDQ0WAcNK4Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest"
      ],
      "metadata": {
        "id": "V9JKyi-3gxfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = ensemble.RandomForestClassifier()\n",
        "#rf = make_pipeline(StandardScaler(), RandomForestClassifier())\n",
        "rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ri4Rd4omYFgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "rfc_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "rfc_precision = metrics.precision_score(y_test, y_pred)\n",
        "rfc_recall = metrics.recall_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", rfc_accuracy)\n",
        "print(\"Precision:\", rfc_precision)\n",
        "print(\"Recall:\", rfc_recall)"
      ],
      "metadata": {
        "id": "Srf9IHHTYl4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = metrics.confusion_matrix(y_test, y_pred)\n",
        "metrics.ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
      ],
      "metadata": {
        "id": "I9x9Z757aI1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RFC Prediction and Evaluation"
      ],
      "metadata": {
        "id": "9V9A56Z4wMYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_messages = extract_emails_from_mbox(\"/content/emails-enron-legal-mails.mbox\")\n",
        "test_message_bodies = [extract_content_email(message) for message in test_messages]\n",
        "# Apply preprocessing function to emails\n",
        "test_messages_preprocessed = custom_preprocessing(test_message_bodies)\n",
        "test_messages_corpus = [dictionary.doc2bow(text) for text in test_messages_preprocessed]"
      ],
      "metadata": {
        "id": "no0oniQ9ndkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_vector_test = create_vectors_from_messages(test_message_bodies, test_messages_preprocessed,test_messages_corpus)"
      ],
      "metadata": {
        "id": "5qX1yw4gvuRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RFC Prediction\n",
        "X = scaler.transform(all_vector_test)\n",
        "y_pred = rf.predict(X)\n",
        "\n",
        "num_phishing = 0\n",
        "for i in y_pred:\n",
        "  if i == 1:\n",
        "    num_phishing += 1\n",
        "\n",
        "print(\"Number of phishing emails:\", num_phishing)\n",
        "print(\"Number of benign emails:\", len(y_pred) - num_phishing)"
      ],
      "metadata": {
        "id": "P_jDt8n_9gv6",
        "outputId": "d2ef1521-71f7-4928-e561-fc333f726b32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of phishing emails: 4279\n",
            "Number of benign emails: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "email_labels = [0 for i in range(len(y_pred))]\n",
        "email_accuracy = metrics.accuracy_score(email_labels, y_pred)\n",
        "print(email_accuracy)"
      ],
      "metadata": {
        "id": "3QD831FFOddE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "print(test_message_bodies[100])\n",
        "print(test_messages_preprocessed[100])"
      ],
      "metadata": {
        "id": "g4p8P9jww2Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of messages: {}\".format(np.array(all_vector_test).shape))"
      ],
      "metadata": {
        "id": "0iS5PObjw3_8",
        "outputId": "f568a817-cb05-4020-e885-2d1c5476fdf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of messages: (4279, 1060)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVC - Support Vector Classifier"
      ],
      "metadata": {
        "id": "_fkXWY5oBJnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc = svm.SVC(gamma=\"auto\")\n",
        "svc.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "LNDTKkGO8vJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svc.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "precision = metrics.precision_score(y_test, y_pred)\n",
        "recall = metrics.recall_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ],
      "metadata": {
        "id": "-EKUmlaiH2R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = metrics.confusion_matrix(y_test, y_pred)\n",
        "metrics.ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
      ],
      "metadata": {
        "id": "A_Hkxmm5MNRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deep Learning ConvNet (CNN)"
      ],
      "metadata": {
        "id": "psxGPyRRMXFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From: https://keras.io/examples/nlp/text_classification_from_scratch/"
      ],
      "metadata": {
        "id": "oUOOw_zSJRA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf_config.list_physical_devices('GPU'))\n",
        "# tf_config.set_visible_devices([], 'GPU')"
      ],
      "metadata": {
        "id": "uqBpc4fXBbjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (1, len(X_train[0]), 1) # Batch size is the first number\n",
        "print(input_shape)\n",
        "\n",
        "# #Scale data\n",
        "# X_train = all_vectors\n",
        "# scaler = sk_preprocessing.StandardScaler()\n",
        "# scaler.fit(X_train)\n",
        "#Đã có scaler trong classification\n",
        "X_train = scaler.transform(all_vectors)\n",
        "\n",
        "X_train = np.reshape(X_train, (len(X_train), input_shape[1], input_shape[2]))\n",
        "\n",
        "print(X_train.shape)\n",
        "y_train = np.reshareshape(all_labels, (len(X_train), 1))\n",
        "\n",
        "#Split data\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_train, y_train, test_size=0.2, shuffle=True)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "d9RpmaZxDO29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 1024\n",
        "num_layers = 3\n",
        "inputs = tf_keras.Input(shape=input_shape[1:], dtype = \"float32\")\n",
        "x = tf_keras.layers.Conv1D(filters=embedding_dim, kernel_size=2,padding=\"valid\", activation=\"relu\", strides=3)(inputs)\n",
        "x = tf_keras.layers.Dropout(0.5)(x)\n",
        "x = tf_keras.layers.Conv1D(filters=embedding_dim, kernel_size=2,padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = tf_keras.layers.Dropout(0.5)(x)\n",
        "x = tf_keras.layers.Conv1D(filters=embedding_dim, kernel_size=2,padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = tf_keras.layers.GlobalMaxPooling1D()(x)\n",
        "x = tf_keras.layers.Dense(embedding_dim, activation=\"relu\")(x)\n",
        "x = tf_keras.layers.Dropout(0.5)(x)\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = tf_keras.layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf_keras.Model(inputs, predictions)\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"binary_accuracy\"])"
      ],
      "metadata": {
        "id": "Xlu3lB8gGVr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 128\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "id": "fH1DMH-jMekk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN Prediction and Evaluation"
      ],
      "metadata": {
        "id": "9Q7TOAMVL73F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_test = scaler.transform(all_vector_test)\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "9deU01hYMxis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_bin = []\n",
        "num_phishing_CNN = 0\n",
        "for i in y_pred:\n",
        "  if i >= 0.5:\n",
        "    y_pred_bin.append(1)\n",
        "    num_phishing_CNN += 1\n",
        "  else:\n",
        "    y_pred_bin.append(0)\n",
        "\n",
        "print(\"Number of phishing emails:\", num_phishing_CNN)\n",
        "print(\"Number of benign emails:\", len(y_pred) - num_phishing_CNN)"
      ],
      "metadata": {
        "id": "bw4g51OJNdQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_accuracy_CNN  = metrics.accuracy_score(email_labels, y_pred_bin)\n",
        "print(email_accuracy_CNN)"
      ],
      "metadata": {
        "id": "NvMURGmLOFss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Tracking"
      ],
      "metadata": {
        "id": "dlXkYb3IfShK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions: Save stats"
      ],
      "metadata": {
        "id": "eATOFxctfkUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_training_stats(num_topics,max_term, X_train, X_test,\n",
        "                          rfc_accuracy, rfc_recall, email_accuracy,\n",
        "                          embedding_dim, num_layers, history, email_accuracy_CNN):\n",
        "  \"\"\"\n",
        "  Tạo dictionary chứa số liệu thông kê từ kết quả huấn luyện.\n",
        "  Parameters:\n",
        "  - num_topics (int): Số lượng chủ đề.\n",
        "  - max_term (int): Số lượng từ tối đa trong mô hình TF-IDF.\n",
        "  - X_train (list): Dữ liệu huấn luyện.\n",
        "  - X_test (list): Dữ liệu kiểm tra.\n",
        "  - rfc_accuracy (float): Độ chính xác của mô hình RFC.\n",
        "  - rfc_recall (float): Độ hồi phục của mô hình RFC.\n",
        "  - email_accuracy (float): Độ chính xác của mô hình trên GMail (RFC).\n",
        "  - embedding_dim (int): Kích thước không gian nhúng của mô hình CNN.\n",
        "  - num_layers (int): Số lượng lớp trong mô hình CNN.\n",
        "  - history (History): Lịch sử huấn luyện của mô hình CNN.\n",
        "  - email_accuracy_CNN (float): Độ chính xác của mô hình CNN trên GMail.\n",
        "\n",
        "  Returns: dictionary chứa dữ liệu thống kê.\n",
        "  \"\"\"\n",
        "  stats_data ={\n",
        "      \"Num Topics\" : [num_topics],\n",
        "      \"TF-IDF Count\": [max_term],\n",
        "      \"Num Training\" : [len(X_train)],\n",
        "      \"Num Test\" : [len(X_test)],\n",
        "      \"RFC Test Accuracy\" : [rfc_accuracy],\n",
        "      \"RFC Test Recall\" : [rfc_recall],\n",
        "      \"RFC GMail Accuracy\" : [email_accuracy],\n",
        "      \"CNN Dim\" : [embedding_dim],\n",
        "      \"CNN Num Layers\" : [num_layers],\n",
        "      \"CNN Accuracy\" : [history.history[\"binary_accuracy\"][-1]],\n",
        "      \"CNN Validation Accuracy\" : [history.history[\"val_binary_accuracy\"][-1]],\n",
        "      \"CNN GMail Accuracy\" : [email_accuracy_CNN]\n",
        "      }\n",
        "  return stats_data\n",
        "\n",
        "\n",
        "def save_stats(stats_data, stats_file_name='stats_results.csv'):\n",
        "  \"\"\"\n",
        "  Lưu số liệu thống kê vào file CSV.\n",
        "\n",
        "  Parameters:\n",
        "  - stats (dict): Một dictionary chứa các kết quả cần lưu, ví dụ: {\"accuracy\": [0.95], \"loss\": [0.1]}.\n",
        "  - file_name (str): Tên file CSV nơi lưu kết quả. Mặc định là 'stats_results.csv'.\n",
        "\n",
        "  Returns: None\n",
        "  \"\"\"\n",
        "  data_frame = pd.DataFrame(stats_data)\n",
        "\n",
        "  if os.path.isfile(stats_file_name):\n",
        "    data_frame.to_csv(stats_file_name, mode='a', header=False, index=False)\n",
        "  else:\n",
        "    data_frame.to_csv(stats_file_name, mode='w', header=True, index=False)\n",
        "\n",
        "  print(f\"Stats saved to {stats_file_name}\")\n"
      ],
      "metadata": {
        "id": "HD4XwG6wPEVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process: Save stats from results"
      ],
      "metadata": {
        "id": "9LyN2JFHu8fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats_data = create_training_stats(numTopics, max_term,\n",
        "                                   X_train, X_test, rfc_accuracy,\n",
        "                                   rfc_recall, email_accuracy,\n",
        "                                   embedding_dim, num_layers,\n",
        "                                   history, email_accuracy_CNN)\n",
        "print(stats_data)"
      ],
      "metadata": {
        "id": "j3wKgrt9uaHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats_file_name = \"/results/stats_results.csv\"\n",
        "save_stats(stats_data, stats_file_name)"
      ],
      "metadata": {
        "id": "Nbb22p1jukuK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}